{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IONet model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in c:\\program files\\python311\\lib\\site-packages (from timm) (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\program files\\python311\\lib\\site-packages (from timm) (0.17.1)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\python311\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\program files\\python311\\lib\\site-packages (from timm) (0.21.4)\n",
      "Requirement already satisfied: safetensors in c:\\program files\\python311\\lib\\site-packages (from timm) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface_hub->timm) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\program files\\python311\\lib\\site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface_hub->timm) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\program files\\python311\\lib\\site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.9.16\n",
      "Requirement already satisfied: transformers in c:\\program files\\python311\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\program files\\python311\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\program files\\python311\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install timm\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import tqdm\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# to get CFIAR10 dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# to import pretrained models\n",
    "from transformers import AutoImageProcessor, MobileNetV1Model\n",
    "import timm\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtImg(img):\n",
    "    img = img.permute([0, 2, 3, 1])\n",
    "    img = img - img.min()\n",
    "    img = (img / img.max())\n",
    "    return img.numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def show_examples(x):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    imgs = cvtImg(x)\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, group=1):\n",
    "        assert group > 1\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        self.group = group\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"https://github.com/Randl/ShuffleNetV2-pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        batchsize, num_channels, height, width = x.data.size()\n",
    "        assert (num_channels % self.group == 0)\n",
    "        channels_per_group = num_channels // self.group\n",
    "        # reshape\n",
    "        x = x.view(batchsize, self.group, channels_per_group, height, width)\n",
    "        # transpose\n",
    "        # - contiguous() required if transpose() is used before view().\n",
    "        #   See https://github.com/pytorch/pytorch/issues/764\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        # flatten\n",
    "        x = x.view(batchsize, -1, height, width)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class FBdecoderLayer(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride,\n",
    "                 expansion, group, bn=False):\n",
    "        super(FBdecoderLayer, self).__init__()\n",
    "        assert not bn, \"not support bn for now\"\n",
    "        bias_flag = not bn\n",
    "        if kernel_size == 1:\n",
    "            padding = 0\n",
    "        elif kernel_size == 3:\n",
    "            padding = 1\n",
    "        elif kernel_size == 5:\n",
    "            padding = 2\n",
    "        elif kernel_size == 7:\n",
    "            padding = 3\n",
    "        else:\n",
    "            raise ValueError(\"Not supported kernel_size %d\" % kernel_size)\n",
    "        if group == 1:\n",
    "            self.op = nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out*expansion, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out*expansion, kernel_size, stride=stride,\n",
    "                          padding=padding, groups=C_out*expansion, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag)\n",
    "            )\n",
    "        else:\n",
    "            self.op = nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out*expansion, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                ChannelShuffle(group),\n",
    "                nn.Conv2d(C_out*expansion, C_out*expansion, kernel_size, stride=stride,\n",
    "                          padding=padding, groups=C_out*expansion, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                ChannelShuffle(group)\n",
    "            )\n",
    "        res_flag = ((C_in == C_out) and (stride == 1))\n",
    "        self.res_flag = res_flag\n",
    "        if not res_flag:\n",
    "            if stride == 2:\n",
    "                self.trans = nn.Conv2d(C_in, C_out, 3, stride=2,\n",
    "                                       padding=1)\n",
    "            elif stride == 1:\n",
    "                self.trans = nn.Conv2d(C_in, C_out, 1, stride=1,\n",
    "                                       padding=0)\n",
    "            else:\n",
    "                raise ValueError(\"Wrong stride %d provided\" % stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.res_flag:\n",
    "            return self.op(x) + x\n",
    "        else:\n",
    "            return self.op(x) + self.trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "FBdecoderLayer(\n",
      "  (op): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (trans): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FBdecoderLayer(C_in=256, C_out=256, kernel_size=3,\n",
    "                       stride=1, expansion=1, group=1, bn=False)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 256, 32, 32)\n",
    "\n",
    "# forward\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "model = FBdecoderLayer(C_in=512, C_out=256, kernel_size=3,\n",
    "                       stride=1, expansion=1, group=1, bn=False)\n",
    "cnn_model = nn.Conv2d(512, 256, 3, stride=1, padding=1)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 512, 32, 32)\n",
    "\n",
    "# forward\n",
    "y = model(x)\n",
    "y_cnn = cnn_model(x)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_cnn.shape)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, group=1):\n",
    "        assert group > 1\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        self.group = group\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"https://github.com/Randl/ShuffleNetV2-pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        batchsize, num_channels, height, width = x.data.size()\n",
    "        assert (num_channels % self.group == 0)\n",
    "        channels_per_group = num_channels // self.group\n",
    "        # reshape\n",
    "        x = x.view(batchsize, self.group, channels_per_group, height, width)\n",
    "        # transpose\n",
    "        # - contiguous() required if transpose() is used before view().\n",
    "        #   See https://github.com/pytorch/pytorch/issues/764\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        # flatten\n",
    "        x = x.view(batchsize, -1, height, width)\n",
    "        return x\n",
    "\n",
    "\n",
    "class identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# based on FB block\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=3):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels*expansion, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels*expansion, in_channels*expansion, kernel_size=5, padding=2, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels*expansion,\n",
    "                               out_channels, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # self.skip_connection = None\n",
    "\n",
    "        # if in_channels != out_channels:\n",
    "        #   self.skip_connection = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1)\n",
    "        # else:\n",
    "        #   self.skip_connection = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_x = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # TODO add skip connection\n",
    "\n",
    "        # print(\"layer output:\", x.shape)\n",
    "        # print(\"skip connection:\", self.skip_connection(in_x).shape)\n",
    "\n",
    "        # add skip connection\n",
    "        # x = x + self.skip_connection(in_x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 256, 32, 32)\n",
    "\n",
    "# forward\n",
    "model = DecoderLayer(256, 256)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MobileNetV2: Inverted Residuals and Linear Bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV1Model(\n",
       "  (conv_stem): MobileNetV1ConvLayer(\n",
       "    (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (layer): ModuleList(\n",
       "    (0): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "      (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (1): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (2): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
       "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (3): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (4): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (5): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (6): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (7): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (8): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (9): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (10): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (11): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (12): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (13): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (14): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (15): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (16): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (17): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (18): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (19): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (20): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (21): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (22): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (23): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (24): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (25): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "  )\n",
       "  (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the MobileNetV2 model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"google/mobilenet_v1_1.0_224\")\n",
    "mobilenet_v1_model = MobileNetV1Model.from_pretrained(\n",
    "    \"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "mobilenet_v1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV1Model(\n",
       "  (conv_stem): MobileNetV1ConvLayer(\n",
       "    (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (layer): ModuleList(\n",
       "    (0): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "      (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (1): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (2): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
       "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (3): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (4): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (5): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (6): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
       "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (7): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (8): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (9): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (10): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (11): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (12): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (13): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (14): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (15): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (16): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (17): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (18): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (19): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (20): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (21): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (22): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (23): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (24): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "    (25): MobileNetV1ConvLayer(\n",
       "      (convolution): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "      (activation): ReLU6()\n",
       "    )\n",
       "  )\n",
       "  (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "mobilenet_v1_model.save_pretrained(\"../Pretrained_models/mobilenet_v1\")\n",
    "\n",
    "mobilenet_v1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV1ConvLayer(\n",
      "  (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU6()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get first layer\n",
    "first_layer = mobilenet_v1_model.conv_stem\n",
    "\n",
    "print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0:\n",
      "  Layer 0: Input: 32, Output: 32, Kernel: (3, 3)\n",
      "  Layer 1: Input: 32, Output: 64, Kernel: (1, 1)\n",
      "  Layer 2: Input: 64, Output: 64, Kernel: (3, 3)\n",
      "  Layer 3: Input: 64, Output: 128, Kernel: (1, 1)\n",
      "Block 1:\n",
      "  Layer 4: Input: 128, Output: 128, Kernel: (3, 3)\n",
      "  Layer 5: Input: 128, Output: 128, Kernel: (1, 1)\n",
      "  Layer 6: Input: 128, Output: 128, Kernel: (3, 3)\n",
      "  Layer 7: Input: 128, Output: 256, Kernel: (1, 1)\n",
      "Block 2:\n",
      "  Layer 8: Input: 256, Output: 256, Kernel: (3, 3)\n",
      "  Layer 9: Input: 256, Output: 256, Kernel: (1, 1)\n",
      "  Layer 10: Input: 256, Output: 256, Kernel: (3, 3)\n",
      "  Layer 11: Input: 256, Output: 512, Kernel: (1, 1)\n",
      "Block 3:\n",
      "  Layer 12: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 13: Input: 512, Output: 512, Kernel: (1, 1)\n",
      "  Layer 14: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 15: Input: 512, Output: 512, Kernel: (1, 1)\n",
      "Block 4:\n",
      "  Layer 16: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 17: Input: 512, Output: 512, Kernel: (1, 1)\n",
      "  Layer 18: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 19: Input: 512, Output: 512, Kernel: (1, 1)\n",
      "Block 5:\n",
      "  Layer 20: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 21: Input: 512, Output: 512, Kernel: (1, 1)\n",
      "  Layer 22: Input: 512, Output: 512, Kernel: (3, 3)\n",
      "  Layer 23: Input: 512, Output: 1024, Kernel: (1, 1)\n",
      "Block 6:\n",
      "  Layer 24: Input: 1024, Output: 1024, Kernel: (3, 3)\n",
      "  Layer 25: Input: 1024, Output: 1024, Kernel: (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "     (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
       "     (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "     (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
       "     (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "     (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "     (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (2): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "     (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (3): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
       "     (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       "   (1): MobileNetV1ConvLayer(\n",
       "     (convolution): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
       "     (activation): ReLU6()\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = mobilenet_v1_model\n",
    "\n",
    "# Define the number of layers per block\n",
    "layers_per_block = 4\n",
    "\n",
    "# Calculate the total number of blocks\n",
    "total_layers = len(model.layer)\n",
    "num_blocks = total_layers // layers_per_block\n",
    "\n",
    "mobilenet_seq_blocks = []\n",
    "\n",
    "# Iterate through the layers and divide them into blocks\n",
    "for block_idx in range(num_blocks):\n",
    "    start_idx = block_idx * layers_per_block\n",
    "    end_idx = (block_idx + 1) * layers_per_block\n",
    "    block_layers = model.layer[start_idx:end_idx]\n",
    "\n",
    "    # Create a block\n",
    "    block = nn.Sequential(*block_layers)\n",
    "    setattr(model, f\"block{block_idx}\", block)\n",
    "\n",
    "    print(f\"Block {block_idx}:\")\n",
    "\n",
    "    # Print input, output, and kernel size for each layer\n",
    "    for layer_idx, layer in enumerate(block, start=start_idx):\n",
    "        if isinstance(layer.convolution, nn.Conv2d):\n",
    "            print(f\"  Layer {layer_idx}: Input: {layer.convolution.in_channels}, Output: {layer.convolution.out_channels}, Kernel: {layer.convolution.kernel_size}\")\n",
    "\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "# add the remaining layers\n",
    "start_idx = num_blocks * layers_per_block\n",
    "\n",
    "# Create a block\n",
    "block = nn.Sequential(*model.layer[start_idx:])\n",
    "setattr(model, f\"block{num_blocks}\", block)\n",
    "\n",
    "print(f\"Block {num_blocks}:\")\n",
    "for layer_idx, layer in enumerate(block, start=start_idx):\n",
    "    if isinstance(layer.convolution, nn.Conv2d):\n",
    "        print(f\"  Layer {layer_idx}: Input: {layer.convolution.in_channels}, Output: {layer.convolution.out_channels}, Kernel: {layer.convolution.kernel_size}\")\n",
    "\n",
    "mobilenet_seq_blocks.append(block)\n",
    "\n",
    "mobilenet_seq_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unet class\n",
    "\n",
    "en_stim: 3 -> 32\n",
    "\n",
    "en0: 32 -> 128\n",
    "\n",
    "en1: 128 -> 256\n",
    "\n",
    "en2: 256 -> 512\n",
    "\n",
    "en3: 512 -> 512\n",
    "\n",
    "en4: 512 -> 512\n",
    "\n",
    "en5: 512 -> 1024\n",
    "\n",
    "en6: 1024 -> 1024\n",
    "\n",
    "de6: 1024 -> 1024\n",
    "\n",
    "de5: 1024 + 1024 -> 512\n",
    "\n",
    "de4: 512 + 512 -> 512\n",
    "\n",
    "de3: 512 + 512 -> 512\n",
    "\n",
    "de2: 512 + 512 -> 256\n",
    "\n",
    "de1: 256 + 256 -> 128\n",
    "\n",
    "de0: 128 + 128 -> 32\n",
    "\n",
    "out_stim: 32 -> 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Unet, self).__init__()\n",
    "\n",
    "\t\t# Encoder layers\n",
    "\t\t\n",
    "\t\t# fist layer is moblienet_v1 layer 1\n",
    "\t\tself.en_stim = mobilenet_v1_model.conv_stem\n",
    "\t\t\n",
    "\t\t# now we have the blocks\n",
    "\t\tself.en0 = mobilenet_seq_blocks[0]\n",
    "\t\tself.en1 = mobilenet_seq_blocks[1]\n",
    "\t\tself.en2 = mobilenet_seq_blocks[2]\n",
    "\t\tself.en3 = mobilenet_seq_blocks[3]\n",
    "\t\tself.en4 = mobilenet_seq_blocks[4]\n",
    "\t\tself.en5 = mobilenet_seq_blocks[5]\n",
    "\t\tself.en6 = mobilenet_seq_blocks[6]\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tself.de6 = DecoderLayer(1024, 1024)\n",
    "\t\tself.de5 = DecoderLayer(1024+1024, 512)\n",
    "\t\tself.de4 = DecoderLayer(512+512, 256)\n",
    "\t\tself.de3 = DecoderLayer(256+256, 128)\n",
    "\t\tself.de2 = DecoderLayer(128+128, 64)\n",
    "\t\tself.de1 = DecoderLayer(64+64, 32)\n",
    "\t\tself.de0 = DecoderLayer(32+32, 32)\n",
    "\n",
    "\t\t# self.de6 = FBdecoderLayer(C_in=1024,C_out=1024,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de5 = FBdecoderLayer(C_in=1024+1024,C_out=512,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de4 = FBdecoderLayer(C_in=512+512,C_out=256,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de3 = FBdecoderLayer(C_in=256+256,C_out=128,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de2 = FBdecoderLayer(C_in=128+128,C_out=64,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de1 = FBdecoderLayer(C_in=64+64,C_out=32,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de0 = FBdecoderLayer(C_in=32+32,C_out=32,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t\n",
    "\t\t# last layer\n",
    "\t\tself.out_stim = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(32, 3, kernel_size=3, stride=2, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(3),\n",
    "\t\t\tnn.ReLU6()\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Output layer\n",
    "\t\tself.out_stim = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Encoder pass\n",
    "\t\ten_stim_out = F.relu(self.en_stim(x))\n",
    "\t\tprint(\"en_stim_out shape:\", en_stim_out.shape)\n",
    "\n",
    "\t\ten0_out = F.relu(self.en0(en_stim_out))\n",
    "\t\tprint(\"en0_out shape:\", en0_out.shape)\n",
    "\n",
    "\t\ten1_out = F.relu(self.en1(en0_out))\n",
    "\t\tprint(\"en1_out shape:\", en1_out.shape)\n",
    "\n",
    "\t\ten2_out = F.relu(self.en2(en1_out))\n",
    "\t\tprint(\"en2_out shape:\", en2_out.shape)\n",
    "\n",
    "\t\ten3_out = F.relu(self.en3(en2_out))\n",
    "\t\tprint(\"en3_out shape:\", en3_out.shape)\n",
    "\n",
    "\t\ten4_out = F.relu(self.en4(en3_out))\n",
    "\t\tprint(\"en4_out shape:\", en4_out.shape)\n",
    "\n",
    "\t\ten5_out = F.relu(self.en5(en4_out))\n",
    "\t\tprint(\"en5_out shape:\", en5_out.shape)\n",
    "\n",
    "\t\ten6_out = F.relu(self.en6(en5_out))\n",
    "\t\tprint(\"en6_out shape:\", en6_out.shape)\n",
    "\n",
    "\t\t# Decoder pass with skip connections\n",
    "\t\tde6_out = F.relu(self.de6(en6_out))\n",
    "\t\tprint(\"\\nde6_out shape:\", de6_out.shape)\n",
    "\t\tprint(\"en5_out shape:\", en5_out.shape)\n",
    "  \n",
    "\t\tprint(\"de5_in shape:\", torch.cat([de6_out, en5_out], dim=1).shape)\n",
    "\n",
    "\t\tde5_out = F.relu(self.de5(torch.cat([de6_out, en5_out], dim=1)))\n",
    "\t\tprint(\"de5_out shape:\", de5_out.shape)\n",
    "\n",
    "\t\tprint(\"en4_out shape:\", en4_out.shape)\n",
    "\t\tprint(\"de5_in shape:\", torch.cat([de5_out, en4_out], dim=1).shape)\n",
    "\t\tde4_out = F.relu(self.de4(torch.cat([de5_out, en4_out], dim=1)))\n",
    "\t\tprint(\"de4_out shape:\", de4_out.shape)\n",
    "\n",
    "\t\tde3_out = F.relu(self.de3(torch.cat([de4_out, en3_out], dim=1)))\n",
    "\t\tprint(\"de3_out shape:\", de3_out.shape)\n",
    "\n",
    "\t\tde2_out = F.relu(self.de2(torch.cat([de3_out, en2_out], dim=1)))\n",
    "\t\tprint(\"de2_out shape:\", de2_out.shape)\n",
    "\n",
    "\t\tde1_out = F.relu(self.de1(torch.cat([de2_out, en1_out], dim=1)))\n",
    "\t\tprint(\"de1_out shape:\", de1_out.shape)\n",
    "\n",
    "\t\tde0_out = F.relu(self.de0(torch.cat([de1_out, en0_out], dim=1)))\n",
    "\t\tprint(\"de0_out shape:\", de0_out.shape)\n",
    "\n",
    "\t\t# Output prediction\n",
    "\t\toutput = self.out_stim(de0_out)\n",
    "\t\tprint(\"output shape:\", output.shape)\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "\t# def forward(self, x):\n",
    "\t#     # Encoder pass\n",
    "\t#     en_stim_out = F.relu(self.en_stim(x))\n",
    "\t\t\n",
    "\t#     en0_out = F.relu(self.en0(en_stim_out))\n",
    "\t#     en1_out = F.relu(self.en1(en0_out))\n",
    "\t#     en2_out = F.relu(self.en2(en1_out))\n",
    "\t#     en3_out = F.relu(self.en3(en2_out))\n",
    "\t#     en4_out = F.relu(self.en4(en3_out))\n",
    "\t#     en5_out = F.relu(self.en5(en4_out))\n",
    "\t#     en6_out = F.relu(self.en6(en5_out))\n",
    "\n",
    "\t#     # Decoder pass with skip connections\n",
    "\t#     de6_out = F.relu(self.de6(en6_out))\n",
    "\t#     de5_out = F.relu(self.de5(torch.cat([de6_out, en5_out], dim=1)))\n",
    "\t#     de4_out = F.relu(self.de4(torch.cat([de5_out, en4_out], dim=1)))\n",
    "\t#     de3_out = F.relu(self.de3(torch.cat([de4_out, en3_out], dim=1)))\n",
    "\t#     de2_out = F.relu(self.de2(torch.cat([de3_out, en2_out], dim=1)))\n",
    "\t#     de1_out = F.relu(self.de1(torch.cat([de2_out, en1_out], dim=1)))\n",
    "\t#     de0_out = F.relu(self.de0(torch.cat([de1_out, en0_out], dim=1)))\n",
    "\n",
    "\t#     # Output prediction\n",
    "\t#     output = self.out_stim(de0_out)\n",
    "\t#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_stim_out shape: torch.Size([1, 32, 112, 112])\n",
      "en0_out shape: torch.Size([1, 128, 56, 56])\n",
      "en1_out shape: torch.Size([1, 256, 28, 28])\n",
      "en2_out shape: torch.Size([1, 512, 14, 14])\n",
      "en3_out shape: torch.Size([1, 512, 14, 14])\n",
      "en4_out shape: torch.Size([1, 512, 14, 14])\n",
      "en5_out shape: torch.Size([1, 1024, 7, 7])\n",
      "en6_out shape: torch.Size([1, 1024, 7, 7])\n",
      "\n",
      "de6_out shape: torch.Size([1, 1024, 14, 14])\n",
      "en5_out shape: torch.Size([1, 1024, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 14 but got size 7 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 77\u001b[0m, in \u001b[0;36mUnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mde6_out shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, de6_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men5_out shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, en5_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde5_in shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mde6_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men5_out\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m de5_out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mde5(torch\u001b[38;5;241m.\u001b[39mcat([de6_out, en5_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde5_out shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, de5_out\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 14 but got size 7 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model = Unet().to(device)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# forward\n",
    "y = model(x.to(device))\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## FB net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNormAct2d(\n",
       "    16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(672, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(184, 1104, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1104, 1104, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1104, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(1104, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(184, 1104, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1104, 1104, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1104, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(1104, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(184, 1104, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1104, 1104, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1104, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(1104, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          184, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(184, 1104, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1104, 1104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1104, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(1104, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          352, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(352, 1984, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNormAct2d(\n",
       "    1984, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=1984, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbnet_100_model = timm.create_model('fbnetc_100', pretrained=True)\n",
    "\n",
    "fbnet_100_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "fbnet_100_model.save_pretrained(\"../Pretrained_models/fbnet_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
