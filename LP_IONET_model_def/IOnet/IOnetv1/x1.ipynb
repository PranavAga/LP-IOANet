{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IONet model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install timm\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import tqdm\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# to get CFIAR10 dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# to import pretrained models\n",
    "from transformers import AutoImageProcessor, MobileNetV1Model\n",
    "import timm\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtImg(img):\n",
    "    img = img.permute([0, 2, 3, 1])\n",
    "    img = img - img.min()\n",
    "    img = (img / img.max())\n",
    "    return img.numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def show_examples(x):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    imgs = cvtImg(x)\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, group=1):\n",
    "        assert group > 1\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        self.group = group\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"https://github.com/Randl/ShuffleNetV2-pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        batchsize, num_channels, height, width = x.data.size()\n",
    "        assert (num_channels % self.group == 0)\n",
    "        channels_per_group = num_channels // self.group\n",
    "        # reshape\n",
    "        x = x.view(batchsize, self.group, channels_per_group, height, width)\n",
    "        # transpose\n",
    "        # - contiguous() required if transpose() is used before view().\n",
    "        #   See https://github.com/pytorch/pytorch/issues/764\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        # flatten\n",
    "        x = x.view(batchsize, -1, height, width)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class FBdecoderLayer(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride,\n",
    "                 expansion, group, bn=False):\n",
    "        super(FBdecoderLayer, self).__init__()\n",
    "        assert not bn, \"not support bn for now\"\n",
    "        bias_flag = not bn\n",
    "        if kernel_size == 1:\n",
    "            padding = 0\n",
    "        elif kernel_size == 3:\n",
    "            padding = 1\n",
    "        elif kernel_size == 5:\n",
    "            padding = 2\n",
    "        elif kernel_size == 7:\n",
    "            padding = 3\n",
    "        else:\n",
    "            raise ValueError(\"Not supported kernel_size %d\" % kernel_size)\n",
    "        if group == 1:\n",
    "            self.op = nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out*expansion, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out*expansion, kernel_size, stride=stride,\n",
    "                          padding=padding, groups=C_out*expansion, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag)\n",
    "            )\n",
    "        else:\n",
    "            self.op = nn.Sequential(\n",
    "                nn.Conv2d(C_in, C_out*expansion, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                ChannelShuffle(group),\n",
    "                nn.Conv2d(C_out*expansion, C_out*expansion, kernel_size, stride=stride,\n",
    "                          padding=padding, groups=C_out*expansion, bias=bias_flag),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Conv2d(C_out*expansion, C_out, 1, stride=1, padding=0,\n",
    "                          groups=group, bias=bias_flag),\n",
    "                ChannelShuffle(group)\n",
    "            )\n",
    "        res_flag = ((C_in == C_out) and (stride == 1))\n",
    "        self.res_flag = res_flag\n",
    "        if not res_flag:\n",
    "            if stride == 2:\n",
    "                self.trans = nn.Conv2d(C_in, C_out, 3, stride=2,\n",
    "                                       padding=1)\n",
    "            elif stride == 1:\n",
    "                self.trans = nn.Conv2d(C_in, C_out, 1, stride=1,\n",
    "                                       padding=0)\n",
    "            else:\n",
    "                raise ValueError(\"Wrong stride %d provided\" % stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.res_flag:\n",
    "            return self.op(x) + x\n",
    "        else:\n",
    "            return self.op(x) + self.trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FBdecoderLayer(C_in=256, C_out=256, kernel_size=3,\n",
    "                       stride=1, expansion=1, group=1, bn=False)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 256, 32, 32)\n",
    "\n",
    "# forward\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "model = FBdecoderLayer(C_in=512, C_out=256, kernel_size=3,\n",
    "                       stride=1, expansion=1, group=1, bn=False)\n",
    "cnn_model = nn.Conv2d(512, 256, 3, stride=1, padding=1)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 512, 32, 32)\n",
    "\n",
    "# forward\n",
    "y = model(x)\n",
    "y_cnn = cnn_model(x)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_cnn.shape)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, group=1):\n",
    "        assert group > 1\n",
    "        super(ChannelShuffle, self).__init__()\n",
    "        self.group = group\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"https://github.com/Randl/ShuffleNetV2-pytorch/blob/master/model.py\n",
    "        \"\"\"\n",
    "        batchsize, num_channels, height, width = x.data.size()\n",
    "        assert (num_channels % self.group == 0)\n",
    "        channels_per_group = num_channels // self.group\n",
    "        # reshape\n",
    "        x = x.view(batchsize, self.group, channels_per_group, height, width)\n",
    "        # transpose\n",
    "        # - contiguous() required if transpose() is used before view().\n",
    "        #   See https://github.com/pytorch/pytorch/issues/764\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        # flatten\n",
    "        x = x.view(batchsize, -1, height, width)\n",
    "        return x\n",
    "\n",
    "\n",
    "class identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# based on FB block\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=3):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels*expansion, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels*expansion, in_channels*expansion, kernel_size=5, padding=2, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels*expansion,\n",
    "                               out_channels, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # self.skip_connection = None\n",
    "\n",
    "        # if in_channels != out_channels:\n",
    "        #   self.skip_connection = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1)\n",
    "        # else:\n",
    "        #   self.skip_connection = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_x = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # TODO add skip connection\n",
    "\n",
    "        # print(\"layer output:\", x.shape)\n",
    "        # print(\"skip connection:\", self.skip_connection(in_x).shape)\n",
    "\n",
    "        # add skip connection\n",
    "        # x = x + self.skip_connection(in_x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 256, 32, 32)\n",
    "\n",
    "# forward\n",
    "model = DecoderLayer(256, 256)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MobileNetV2: Inverted Residuals and Linear Bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileNetV2 model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"google/mobilenet_v1_1.0_224\")\n",
    "mobilenet_v1_model = MobileNetV1Model.from_pretrained(\n",
    "    \"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "mobilenet_v1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "mobilenet_v1_model.save_pretrained(\"../Pretrained_models/mobilenet_v1\")\n",
    "\n",
    "mobilenet_v1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first layer\n",
    "first_layer = mobilenet_v1_model.conv_stem\n",
    "\n",
    "print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = mobilenet_v1_model\n",
    "\n",
    "# Define the number of layers per block\n",
    "layers_per_block = 4\n",
    "\n",
    "# Calculate the total number of blocks\n",
    "total_layers = len(model.layer)\n",
    "num_blocks = total_layers // layers_per_block\n",
    "\n",
    "mobilenet_seq_blocks = []\n",
    "\n",
    "# Iterate through the layers and divide them into blocks\n",
    "for block_idx in range(num_blocks):\n",
    "    start_idx = block_idx * layers_per_block\n",
    "    end_idx = (block_idx + 1) * layers_per_block\n",
    "    block_layers = model.layer[start_idx:end_idx]\n",
    "\n",
    "    # Create a block\n",
    "    block = nn.Sequential(*block_layers)\n",
    "    setattr(model, f\"block{block_idx}\", block)\n",
    "\n",
    "    print(f\"Block {block_idx}:\")\n",
    "\n",
    "    # Print input, output, and kernel size for each layer\n",
    "    for layer_idx, layer in enumerate(block, start=start_idx):\n",
    "        if isinstance(layer.convolution, nn.Conv2d):\n",
    "            print(f\"  Layer {layer_idx}: Input: {layer.convolution.in_channels}, Output: {layer.convolution.out_channels}, Kernel: {layer.convolution.kernel_size}\")\n",
    "\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "# add the remaining layers\n",
    "start_idx = num_blocks * layers_per_block\n",
    "\n",
    "# Create a block\n",
    "block = nn.Sequential(*model.layer[start_idx:])\n",
    "setattr(model, f\"block{num_blocks}\", block)\n",
    "\n",
    "print(f\"Block {num_blocks}:\")\n",
    "for layer_idx, layer in enumerate(block, start=start_idx):\n",
    "    if isinstance(layer.convolution, nn.Conv2d):\n",
    "        print(f\"  Layer {layer_idx}: Input: {layer.convolution.in_channels}, Output: {layer.convolution.out_channels}, Kernel: {layer.convolution.kernel_size}\")\n",
    "\n",
    "mobilenet_seq_blocks.append(block)\n",
    "\n",
    "mobilenet_seq_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unet class\n",
    "\n",
    "en_stim: 3 -> 32\n",
    "\n",
    "en0: 32 -> 128\n",
    "\n",
    "en1: 128 -> 256\n",
    "\n",
    "en2: 256 -> 512\n",
    "\n",
    "en3: 512 -> 512\n",
    "\n",
    "en4: 512 -> 512\n",
    "\n",
    "en5: 512 -> 1024\n",
    "\n",
    "en6: 1024 -> 1024\n",
    "\n",
    "de6: 1024 -> 1024\n",
    "\n",
    "de5: 1024 + 1024 -> 512\n",
    "\n",
    "de4: 512 + 512 -> 512\n",
    "\n",
    "de3: 512 + 512 -> 512\n",
    "\n",
    "de2: 512 + 512 -> 256\n",
    "\n",
    "de1: 256 + 256 -> 128\n",
    "\n",
    "de0: 128 + 128 -> 32\n",
    "\n",
    "out_stim: 32 -> 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Unet, self).__init__()\n",
    "\n",
    "\t\t# Encoder layers\n",
    "\t\t\n",
    "\t\t# fist layer is moblienet_v1 layer 1\n",
    "\t\tself.en_stim = mobilenet_v1_model.conv_stem\n",
    "\t\t\n",
    "\t\t# now we have the blocks\n",
    "\t\tself.en0 = mobilenet_seq_blocks[0]\n",
    "\t\tself.en1 = mobilenet_seq_blocks[1]\n",
    "\t\tself.en2 = mobilenet_seq_blocks[2]\n",
    "\t\tself.en3 = mobilenet_seq_blocks[3]\n",
    "\t\tself.en4 = mobilenet_seq_blocks[4]\n",
    "\t\tself.en5 = mobilenet_seq_blocks[5]\n",
    "\t\tself.en6 = mobilenet_seq_blocks[6]\n",
    "\n",
    "\t\t# Decoder layers\n",
    "\t\tself.de6 = DecoderLayer(1024, 1024)\n",
    "\t\tself.de5 = DecoderLayer(1024+1024, 512)\n",
    "\t\tself.de4 = DecoderLayer(512+512, 256)\n",
    "\t\tself.de3 = DecoderLayer(256+256, 128)\n",
    "\t\tself.de2 = DecoderLayer(128+128, 64)\n",
    "\t\tself.de1 = DecoderLayer(64+64, 32)\n",
    "\t\tself.de0 = DecoderLayer(32+32, 32)\n",
    "\n",
    "\t\t# self.de6 = FBdecoderLayer(C_in=1024,C_out=1024,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de5 = FBdecoderLayer(C_in=1024+1024,C_out=512,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de4 = FBdecoderLayer(C_in=512+512,C_out=256,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de3 = FBdecoderLayer(C_in=256+256,C_out=128,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de2 = FBdecoderLayer(C_in=128+128,C_out=64,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de1 = FBdecoderLayer(C_in=64+64,C_out=32,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t# self.de0 = FBdecoderLayer(C_in=32+32,C_out=32,kernel_size=5,stride=1,expansion=1,group=1,bn=False)\n",
    "\t\t\n",
    "\t\t# last layer\n",
    "\t\tself.out_stim = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(32, 3, kernel_size=3, stride=2, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(3),\n",
    "\t\t\tnn.ReLU6()\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Output layer\n",
    "\t\tself.out_stim = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Encoder pass\n",
    "\t\ten_stim_out = F.relu(self.en_stim(x))\n",
    "\t\tprint(\"en_stim_out shape:\", en_stim_out.shape)\n",
    "\n",
    "\t\ten0_out = F.relu(self.en0(en_stim_out))\n",
    "\t\tprint(\"en0_out shape:\", en0_out.shape)\n",
    "\n",
    "\t\ten1_out = F.relu(self.en1(en0_out))\n",
    "\t\tprint(\"en1_out shape:\", en1_out.shape)\n",
    "\n",
    "\t\ten2_out = F.relu(self.en2(en1_out))\n",
    "\t\tprint(\"en2_out shape:\", en2_out.shape)\n",
    "\n",
    "\t\ten3_out = F.relu(self.en3(en2_out))\n",
    "\t\tprint(\"en3_out shape:\", en3_out.shape)\n",
    "\n",
    "\t\ten4_out = F.relu(self.en4(en3_out))\n",
    "\t\tprint(\"en4_out shape:\", en4_out.shape)\n",
    "\n",
    "\t\ten5_out = F.relu(self.en5(en4_out))\n",
    "\t\tprint(\"en5_out shape:\", en5_out.shape)\n",
    "\n",
    "\t\ten6_out = F.relu(self.en6(en5_out))\n",
    "\t\tprint(\"en6_out shape:\", en6_out.shape)\n",
    "\n",
    "\t\t# Decoder pass with skip connections\n",
    "\t\tde6_out = F.relu(self.de6(en6_out))\n",
    "\t\tprint(\"\\nde6_out shape:\", de6_out.shape)\n",
    "\t\tprint(\"en5_out shape:\", en5_out.shape)\n",
    "  \n",
    "\t\tprint(\"de5_in shape:\", torch.cat([de6_out, en5_out], dim=1).shape)\n",
    "\n",
    "\t\tde5_out = F.relu(self.de5(torch.cat([de6_out, en5_out], dim=1)))\n",
    "\t\tprint(\"de5_out shape:\", de5_out.shape)\n",
    "\n",
    "\t\tprint(\"en4_out shape:\", en4_out.shape)\n",
    "\t\tprint(\"de5_in shape:\", torch.cat([de5_out, en4_out], dim=1).shape)\n",
    "\t\tde4_out = F.relu(self.de4(torch.cat([de5_out, en4_out], dim=1)))\n",
    "\t\tprint(\"de4_out shape:\", de4_out.shape)\n",
    "\n",
    "\t\tde3_out = F.relu(self.de3(torch.cat([de4_out, en3_out], dim=1)))\n",
    "\t\tprint(\"de3_out shape:\", de3_out.shape)\n",
    "\n",
    "\t\tde2_out = F.relu(self.de2(torch.cat([de3_out, en2_out], dim=1)))\n",
    "\t\tprint(\"de2_out shape:\", de2_out.shape)\n",
    "\n",
    "\t\tde1_out = F.relu(self.de1(torch.cat([de2_out, en1_out], dim=1)))\n",
    "\t\tprint(\"de1_out shape:\", de1_out.shape)\n",
    "\n",
    "\t\tde0_out = F.relu(self.de0(torch.cat([de1_out, en0_out], dim=1)))\n",
    "\t\tprint(\"de0_out shape:\", de0_out.shape)\n",
    "\n",
    "\t\t# Output prediction\n",
    "\t\toutput = self.out_stim(de0_out)\n",
    "\t\tprint(\"output shape:\", output.shape)\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "\t# def forward(self, x):\n",
    "\t#     # Encoder pass\n",
    "\t#     en_stim_out = F.relu(self.en_stim(x))\n",
    "\t\t\n",
    "\t#     en0_out = F.relu(self.en0(en_stim_out))\n",
    "\t#     en1_out = F.relu(self.en1(en0_out))\n",
    "\t#     en2_out = F.relu(self.en2(en1_out))\n",
    "\t#     en3_out = F.relu(self.en3(en2_out))\n",
    "\t#     en4_out = F.relu(self.en4(en3_out))\n",
    "\t#     en5_out = F.relu(self.en5(en4_out))\n",
    "\t#     en6_out = F.relu(self.en6(en5_out))\n",
    "\n",
    "\t#     # Decoder pass with skip connections\n",
    "\t#     de6_out = F.relu(self.de6(en6_out))\n",
    "\t#     de5_out = F.relu(self.de5(torch.cat([de6_out, en5_out], dim=1)))\n",
    "\t#     de4_out = F.relu(self.de4(torch.cat([de5_out, en4_out], dim=1)))\n",
    "\t#     de3_out = F.relu(self.de3(torch.cat([de4_out, en3_out], dim=1)))\n",
    "\t#     de2_out = F.relu(self.de2(torch.cat([de3_out, en2_out], dim=1)))\n",
    "\t#     de1_out = F.relu(self.de1(torch.cat([de2_out, en1_out], dim=1)))\n",
    "\t#     de0_out = F.relu(self.de0(torch.cat([de1_out, en0_out], dim=1)))\n",
    "\n",
    "\t#     # Output prediction\n",
    "\t#     output = self.out_stim(de0_out)\n",
    "\t#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet().to(device)\n",
    "\n",
    "# random input\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# forward\n",
    "y = model(x.to(device))\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## FB net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbnet_100_model = timm.create_model('fbnetc_100', pretrained=True)\n",
    "\n",
    "fbnet_100_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "fbnet_100_model.save_pretrained(\"../Pretrained_models/fbnet_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
