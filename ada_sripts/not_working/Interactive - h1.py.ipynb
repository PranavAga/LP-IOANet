{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to base (Python 3.12.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sreenivas88/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 DEVICE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Imports\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoImageProcessor, MobileNetV1Model\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# set seed for all possible random functions to ensure reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "print(f\"Using {DEVICE} DEVICE\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 DEVICE\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Imports\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoImageProcessor, MobileNetV1Model\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# set seed for all possible random functions to ensure reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "print(f\"Using {DEVICE} DEVICE\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting encoder layers...\n",
      "([Sequential(\n",
      "  (0): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "    (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (1): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
      "    (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (1): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
      "    (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (1): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (2): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
      "    (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (3): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
      "    (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (1): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (2): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
      "    (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (3): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (1): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (2): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (3): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (4): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (5): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (6): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (7): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (8): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (9): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (10): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
      "    (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (11): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (12): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
      "    (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      "  (13): MobileNetV1ConvLayer(\n",
      "    (convolution): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU6()\n",
      "  )\n",
      ")], MobileNetV1ConvLayer(\n",
      "  (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU6()\n",
      "), MobileNetV1ImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"MobileNetV1ImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 256\n",
      "  }\n",
      "}\n",
      ")\n",
      "Encoder layers retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Model Definition\n",
    "\"\"\"\n",
    "\n",
    "def get_encoder_layers():\n",
    "    \"\"\"\n",
    "    Retrieves the layers of the MobileNetV1 model for encoding images.\n",
    "\n",
    "    Returns:\n",
    "        mobilenet_seq_blocks (list): List containing the layers of the MobileNetV1 model\n",
    "            divided into blocks.\n",
    "        conv_stem (torch.nn.Module): The stem convolutional layer of the MobileNetV1 model.\n",
    "        image_processor (transformers.AutoImageProcessor): Pretrained image processor for\n",
    "            MobileNetV1.\n",
    "    \"\"\"\n",
    "    # download the model\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"google/mobilenet_v1_1.0_224\")\n",
    "    model = MobileNetV1Model.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "    mobilenet_seq_blocks = []\n",
    "    # block 1 will contain 4 layer of model.layer\n",
    "    block = nn.Sequential(*list(model.layer)[:2])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[2:4])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[4:8])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[8:12])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[12:])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "    \n",
    "    return mobilenet_seq_blocks, model.conv_stem, image_processor\n",
    "\n",
    "print(\"Getting encoder layers...\",flush=True)\n",
    "print(get_encoder_layers())\n",
    "print(\"Encoder layers retrieved successfully.\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Model Definition\n",
    "\"\"\"\n",
    "\n",
    "def get_encoder_layers():\n",
    "    \"\"\"\n",
    "    Retrieves the layers of the MobileNetV1 model for encoding images.\n",
    "\n",
    "    Returns:\n",
    "        mobilenet_seq_blocks (list): List containing the layers of the MobileNetV1 model\n",
    "            divided into blocks.\n",
    "        conv_stem (torch.nn.Module): The stem convolutional layer of the MobileNetV1 model.\n",
    "        image_processor (transformers.AutoImageProcessor): Pretrained image processor for\n",
    "            MobileNetV1.\n",
    "    \"\"\"\n",
    "    # download the model\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"google/mobilenet_v1_1.0_224\")\n",
    "    model = MobileNetV1Model.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "    mobilenet_seq_blocks = []\n",
    "    # block 1 will contain 4 layer of model.layer\n",
    "    block = nn.Sequential(*list(model.layer)[:2])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[2:4])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[4:8])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[8:12])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[12:])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "    \n",
    "    return mobilenet_seq_blocks, model.conv_stem, image_processor\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=3, do_up_sampling=True):\n",
    "        # TODO: adding skip connections\n",
    "        \"\"\"\n",
    "        Decoder block module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            expansion (int, optional): Expansion factor. Default is 3.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, in_channels *\n",
    "                              expansion, kernel_size=1, stride=1)\n",
    "        self.bnn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        # nearest neighbor x2\n",
    "        self.do_up_sampling = do_up_sampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # DW conv/ c_in*exp x 5 x 5 x c_in*exp\n",
    "        self.cnn2 = nn.Conv2d(in_channels*expansion, in_channels *\n",
    "                              expansion, kernel_size=5, padding=2, stride=1)\n",
    "        self.bnn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        self.cnn3 = nn.Conv2d(in_channels*expansion,\n",
    "                              out_channels, kernel_size=1, stride=1)\n",
    "        self.bnn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.cnn1(x)\n",
    "        x = self.bnn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.do_up_sampling:\n",
    "            x = self.upsample(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = self.bnn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.cnn3(x)\n",
    "        x = self.bnn3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_decoder_layers(out_sizes=[512, 256, 128, 64, 32]):\n",
    "    decoder_blocks = nn.ModuleList()\n",
    "    for i, out_size in enumerate(out_sizes):\n",
    "        if i == 0:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*2, out_size))\n",
    "        elif i == len(out_sizes)-1:\n",
    "            decoder_blocks.append(DecoderBlock(\n",
    "                out_size*4, out_size, do_up_sampling=False))\n",
    "        else:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*4, out_size))\n",
    "    return decoder_blocks\n",
    "\n",
    "print(get_decoder_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Model Definition\n",
    "\"\"\"\n",
    "\n",
    "def get_encoder_layers():\n",
    "    \"\"\"\n",
    "    Retrieves the layers of the MobileNetV1 model for encoding images.\n",
    "\n",
    "    Returns:\n",
    "        mobilenet_seq_blocks (list): List containing the layers of the MobileNetV1 model\n",
    "            divided into blocks.\n",
    "        conv_stem (torch.nn.Module): The stem convolutional layer of the MobileNetV1 model.\n",
    "        image_processor (transformers.AutoImageProcessor): Pretrained image processor for\n",
    "            MobileNetV1.\n",
    "    \"\"\"\n",
    "    # download the model\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"google/mobilenet_v1_1.0_224\")\n",
    "    model = MobileNetV1Model.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "    mobilenet_seq_blocks = nn.ModuleList()\n",
    "    # block 1 will contain 4 layer of model.layer\n",
    "    block = nn.Sequential(*list(model.layer)[:2])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[2:4])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[4:8])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[8:12])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[12:])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "    \n",
    "    return mobilenet_seq_blocks, model.conv_stem, image_processor\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=3, do_up_sampling=True):\n",
    "        # TODO: adding skip connections\n",
    "        \"\"\"\n",
    "        Decoder block module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            expansion (int, optional): Expansion factor. Default is 3.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, in_channels *\n",
    "                              expansion, kernel_size=1, stride=1)\n",
    "        self.bnn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        # nearest neighbor x2\n",
    "        self.do_up_sampling = do_up_sampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # DW conv/ c_in*exp x 5 x 5 x c_in*exp\n",
    "        self.cnn2 = nn.Conv2d(in_channels*expansion, in_channels *\n",
    "                              expansion, kernel_size=5, padding=2, stride=1)\n",
    "        self.bnn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        self.cnn3 = nn.Conv2d(in_channels*expansion,\n",
    "                              out_channels, kernel_size=1, stride=1)\n",
    "        self.bnn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.cnn1(x)\n",
    "        x = self.bnn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.do_up_sampling:\n",
    "            x = self.upsample(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = self.bnn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.cnn3(x)\n",
    "        x = self.bnn3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_decoder_layers(out_sizes=[512, 256, 128, 64, 32]):\n",
    "    decoder_blocks = nn.ModuleList()\n",
    "    for i, out_size in enumerate(out_sizes):\n",
    "        if i == 0:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*2, out_size))\n",
    "        elif i == len(out_sizes)-1:\n",
    "            decoder_blocks.append(DecoderBlock(\n",
    "                out_size*4, out_size, do_up_sampling=False))\n",
    "        else:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*4, out_size))\n",
    "    return decoder_blocks\n",
    "\n",
    "print(get_decoder_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(3072, 3072, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): DecoderBlock(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (cnn2): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (bnn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cnn3): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bnn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "(ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "      (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (1): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
      "      (normalization): BatchNorm2d(64, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (1): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
      "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (1): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (2): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
      "      (normalization): BatchNorm2d(128, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (3): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
      "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (1): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (2): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
      "      (normalization): BatchNorm2d(256, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (3): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (1): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (2): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (3): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (4): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (5): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (6): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (7): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (8): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (9): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (10): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
      "      (normalization): BatchNorm2d(512, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (11): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (12): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
      "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "    (13): MobileNetV1ConvLayer(\n",
      "      (convolution): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(1024, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU6()\n",
      "    )\n",
      "  )\n",
      "), MobileNetV1ConvLayer(\n",
      "  (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (normalization): BatchNorm2d(32, eps=0.001, momentum=0.9997, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU6()\n",
      "), MobileNetV1ImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"MobileNetV1ImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 256\n",
      "  }\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Model Definition\n",
    "\"\"\"\n",
    "\n",
    "def get_encoder_layers():\n",
    "    \"\"\"\n",
    "    Retrieves the layers of the MobileNetV1 model for encoding images.\n",
    "\n",
    "    Returns:\n",
    "        mobilenet_seq_blocks (list): List containing the layers of the MobileNetV1 model\n",
    "            divided into blocks.\n",
    "        conv_stem (torch.nn.Module): The stem convolutional layer of the MobileNetV1 model.\n",
    "        image_processor (transformers.AutoImageProcessor): Pretrained image processor for\n",
    "            MobileNetV1.\n",
    "    \"\"\"\n",
    "    # download the model\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"google/mobilenet_v1_1.0_224\")\n",
    "    model = MobileNetV1Model.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "    mobilenet_seq_blocks = nn.ModuleList()\n",
    "    # block 1 will contain 4 layer of model.layer\n",
    "    block = nn.Sequential(*list(model.layer)[:2])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[2:4])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[4:8])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[8:12])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "\n",
    "    block = nn.Sequential(*list(model.layer)[12:])\n",
    "    mobilenet_seq_blocks.append(block)\n",
    "    \n",
    "    return mobilenet_seq_blocks, model.conv_stem, image_processor\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=3, do_up_sampling=True):\n",
    "        # TODO: adding skip connections\n",
    "        \"\"\"\n",
    "        Decoder block module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            expansion (int, optional): Expansion factor. Default is 3.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, in_channels *\n",
    "                              expansion, kernel_size=1, stride=1)\n",
    "        self.bnn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        # nearest neighbor x2\n",
    "        self.do_up_sampling = do_up_sampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # DW conv/ c_in*exp x 5 x 5 x c_in*exp\n",
    "        self.cnn2 = nn.Conv2d(in_channels*expansion, in_channels *\n",
    "                              expansion, kernel_size=5, padding=2, stride=1)\n",
    "        self.bnn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "        self.cnn3 = nn.Conv2d(in_channels*expansion,\n",
    "                              out_channels, kernel_size=1, stride=1)\n",
    "        self.bnn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.cnn1(x)\n",
    "        x = self.bnn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.do_up_sampling:\n",
    "            x = self.upsample(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = self.bnn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.cnn3(x)\n",
    "        x = self.bnn3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_decoder_layers(out_sizes=[512, 256, 128, 64, 32]):\n",
    "    decoder_blocks = nn.ModuleList()\n",
    "    for i, out_size in enumerate(out_sizes):\n",
    "        if i == 0:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*2, out_size))\n",
    "        elif i == len(out_sizes)-1:\n",
    "            decoder_blocks.append(DecoderBlock(\n",
    "                out_size*4, out_size, do_up_sampling=False))\n",
    "        else:\n",
    "            decoder_blocks.append(DecoderBlock(out_size*4, out_size))\n",
    "    return decoder_blocks\n",
    "\n",
    "print(get_decoder_layers())\n",
    "print(get_encoder_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ## Model Definition \"\"\"\n",
    "\n",
    "class UnetWithoutAT(nn.Module):\n",
    "    def __init__(self, loss_weights=(10,5), lr=0.5):\n",
    "        super(UnetWithoutAT, self).__init__()\n",
    "        encoder_blocks, image_stem_layer, image_processor = get_encoder_layers()\n",
    "        decoder_blocks = get_decoder_layers()\n",
    "\n",
    "        self.encoder_blocks = encoder_blocks\n",
    "        self.decoder_blocks = decoder_blocks\n",
    "\n",
    "        self.image_processor = image_processor\n",
    "        self.image_stem_layer = image_stem_layer\n",
    "        \n",
    "        self.out_image_stem_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2,\n",
    "                               bias=False, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(3, eps=0.001, momentum=0.9997,\n",
    "                           affine=True, track_running_stats=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.loss_weights = loss_weights\n",
    "        self.loss1 = nn.L1Loss()\n",
    "        # self.loss2 = nn.\n",
    "        \n",
    "        self.metric = SSIM()\n",
    "        \n",
    "\n",
    "    def forward(self, x, process_image=False):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the U-Net model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor.\n",
    "            process_image (bool): Whether to preprocess input image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output image tensor.\n",
    "        \"\"\"\n",
    "        if process_image:\n",
    "            new_x = [self.image_processor(img)['pixel_values'][0] for img in x]\n",
    "            x = torch.stack(new_x).permute(0, 3, 1, 2)\n",
    "        assert x.shape[1] == 3, \"Input image should have 3 channels(nx3x224x224)\"\n",
    "\n",
    "        temp_in_x = x\n",
    "        x = self.image_stem_layer(x)\n",
    "        enc_outputs = []\n",
    "        for indx, enc_block in enumerate(self.encoder_blocks):\n",
    "            x = enc_block(x)\n",
    "            print(f\"Encoder block {indx} output shape: {x.shape}\")\n",
    "            enc_outputs.append(x)\n",
    "        for indx, dec_block in enumerate(self.decoder_blocks):\n",
    "            if indx == 0:\n",
    "                x = dec_block(x)\n",
    "            else:\n",
    "                x = dec_block(\n",
    "                    torch.cat([x, enc_outputs[len(self.decoder_blocks) - indx - 1]], dim=1))\n",
    "            print(f\"Decoder block {indx} output shape: {x.shape}\")\n",
    "        return 0.5 * self.out_image_stem_layer(x) + temp_in_x\n",
    "    \n",
    "    def loss_layer(self,y_pred,y):\n",
    "        '''\n",
    "        Considering wieghted loss\n",
    "        '''\n",
    "        return self.loss_weights[0]*self.loss1(y_pred, y)\n",
    "    \n",
    "    def loss(self,x,y):\n",
    "        with torch.no_grad():\n",
    "            pred = self(x)\n",
    "            return self.loss_layer(pred,y).item()\n",
    "\n",
    "    def accuracy(self,x,y):\n",
    "        with torch.no_grad():\n",
    "            pred = self(x)\n",
    "            return self.metric(pred,y)\n",
    "\n",
    "    def fit(self, x_train, y_train, batch_size=32, n_epochs=3, validation=False, x_val=None, y_val=None):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            #mini-batch gradient descent\n",
    "            for i in range((x_train.shape[0] - 1) // batch_size + 1):\n",
    "                start_i = i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "                xb = x_train[start_i:end_i]\n",
    "                yb = y_train[start_i:end_i]\n",
    "                pred = self(xb)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_layer(pred,yb)\n",
    "               \n",
    "                metrices = {\"train/loss\": loss.item(),\n",
    "                           \"epoch\": epoch+1, \n",
    "                }\n",
    "                if validation:\n",
    "                    metrices['val/loss'] = self.loss(x_val,y_val)\n",
    "                    metrices['val/accuracy'] = self.accuracy(x_val,y_val)\n",
    "                    \n",
    "                wandb.log(metrices)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        wandb.finish()\n",
    "        return self.loss(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ## Model Definition \"\"\"\n",
    "\n",
    "class UnetWithoutAT(nn.Module):\n",
    "    def __init__(self, loss_weights=(10,5), lr=0.5):\n",
    "        super(UnetWithoutAT, self).__init__()\n",
    "        encoder_blocks, image_stem_layer, image_processor = get_encoder_layers()\n",
    "        decoder_blocks = get_decoder_layers()\n",
    "\n",
    "        self.encoder_blocks = encoder_blocks\n",
    "        self.decoder_blocks = decoder_blocks\n",
    "\n",
    "        self.image_processor = image_processor\n",
    "        self.image_stem_layer = image_stem_layer\n",
    "        \n",
    "        self.out_image_stem_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2,\n",
    "                               bias=False, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(3, eps=0.001, momentum=0.9997,\n",
    "                           affine=True, track_running_stats=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.loss_weights = loss_weights\n",
    "        self.loss1 = nn.L1Loss()\n",
    "        # self.loss2 = nn.\n",
    "        \n",
    "        self.metric = SSIM()\n",
    "        \n",
    "\n",
    "    def forward(self, x, process_image=False):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the U-Net model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor.\n",
    "            process_image (bool): Whether to preprocess input image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output image tensor.\n",
    "        \"\"\"\n",
    "        if process_image:\n",
    "            new_x = [self.image_processor(img)['pixel_values'][0] for img in x]\n",
    "            x = torch.stack(new_x).permute(0, 3, 1, 2)\n",
    "        assert x.shape[1] == 3, \"Input image should have 3 channels(nx3x224x224)\"\n",
    "\n",
    "        temp_in_x = x\n",
    "        x = self.image_stem_layer(x)\n",
    "        enc_outputs = []\n",
    "        for indx, enc_block in enumerate(self.encoder_blocks):\n",
    "            x = enc_block(x)\n",
    "            print(f\"Encoder block {indx} output shape: {x.shape}\")\n",
    "            enc_outputs.append(x)\n",
    "        for indx, dec_block in enumerate(self.decoder_blocks):\n",
    "            if indx == 0:\n",
    "                x = dec_block(x)\n",
    "            else:\n",
    "                x = dec_block(\n",
    "                    torch.cat([x, enc_outputs[len(self.decoder_blocks) - indx - 1]], dim=1))\n",
    "            print(f\"Decoder block {indx} output shape: {x.shape}\")\n",
    "        return 0.5 * self.out_image_stem_layer(x) + temp_in_x\n",
    "    \n",
    "    def loss_layer(self,y_pred,y):\n",
    "        '''\n",
    "        Considering wieghted loss\n",
    "        '''\n",
    "        return self.loss_weights[0]*self.loss1(y_pred, y)\n",
    "    \n",
    "    def loss(self,x,y):\n",
    "        with torch.no_grad():\n",
    "            pred = self(x)\n",
    "            return self.loss_layer(pred,y).item()\n",
    "\n",
    "    def accuracy(self,x,y):\n",
    "        with torch.no_grad():\n",
    "            pred = self(x)\n",
    "            return self.metric(pred,y)\n",
    "\n",
    "    def fit(self, train_loader, n_epochs=3, validation=False, val_loader=None, print_every_epoch=True):\n",
    "        \n",
    "        if print_every_epoch: print(\"Training started\")\n",
    "        \n",
    "        for epoch in (range(n_epochs)):\n",
    "            if print_every_epoch: print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            \n",
    "            for x_train, y_train in tqdm(train_loader):\n",
    "                y_pred = self(x_train)\n",
    "                loss = self.loss_layer(y_pred,y_train)\n",
    "                loss.backward()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_layer(y_pred,y_train)\n",
    "                \n",
    "                metrices = {\"train/loss\": loss.item(),\n",
    "                           \"epoch\": epoch+1, \n",
    "                }\n",
    "                if validation:\n",
    "                    metrices['val/loss'] = 0\n",
    "                    metrices['val/accuracy'] = 0\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        metrices['val/loss'] += self.loss(x_val,y_val)\n",
    "                        metrices['val/accuracy'] += self.accuracy(x_val,y_val)\n",
    "                    \n",
    "                    metrices['val/loss'] /= len(val_loader)\n",
    "                    metrices['val/accuracy'] /= len(val_loader)\n",
    "                    \n",
    "                    if print_every_epoch: print(f\"train_loss: {metrices['train/loss']}, val_loss: {metrices['val/loss']}, val_accuracy: {metrices['val/accuracy']}\")\n",
    "\n",
    "                wandb.log(metrices)                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "        if print_every_epoch: print(\"Training finished\")\n",
    "        \n",
    "        wandb.finish()\n",
    "        return self.loss(x_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
