
torch.Size([900, 3, 256, 192]) torch.Size([100, 3, 256, 192]) torch.Size([900, 3, 256, 192]) torch.Size([100, 3, 256, 192])
torch.Size([900, 3, 256, 192]) torch.Size([100, 3, 256, 192]) torch.Size([900, 3, 256, 192]) torch.Size([100, 3, 256, 192])
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/home2/sreenivas88/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home2/sreenivas88/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
model size: 969.097MB
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
Training started
Epoch 1/50












































100%|██████████| 90/90 [01:28<00:00,  1.02it/s]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
Training started
Epoch 1/50



100%|██████████| 5/5 [00:04<00:00,  1.03it/s]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"shape": "(50, 3, 256, 192)", "count": 50, "type": "Tensor"}
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
Training started
Epoch 1/50

 60%|██████    | 3/5 [00:02<00:01,  1.03it/s]
Epoch 0/50: |>train_loss: 13.228235816955566, val_loss: 14.260807991027832, val_accuracy: 0.21629957854747772
100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 2/50

 80%|████████  | 4/5 [00:03<00:00,  1.05it/s]
Epoch 1/50: |>train_loss: 13.209421157836914, val_loss: 14.14029598236084, val_accuracy: 0.22019067406654358
Original Image
100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).


 80%|████████  | 4/5 [00:03<00:00,  1.05it/s]
Epoch 2/50: |>train_loss: 13.589305686950684, val_loss: 14.051461219787598, val_accuracy: 0.22121882438659668
Original Image
100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).


100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Epoch 3/50: |>train_loss: 13.552757835388183, val_loss: 14.06243896484375, val_accuracy: 0.22055597603321075
Original Image
Epoch 5/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).

 60%|██████    | 3/5 [00:03<00:02,  1.00s/it]
Epoch 4/50: |>train_loss: 13.596636962890624, val_loss: 14.086359977722168, val_accuracy: 0.2212793081998825

100%|██████████| 5/5 [00:04<00:00,  1.05it/s]
Epoch 6/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).

 60%|██████    | 3/5 [00:02<00:01,  1.03it/s]
Epoch 5/50: |>train_loss: 13.185140609741211, val_loss: 14.207043647766113, val_accuracy: 0.22133250534534454
Original Image
100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).


 80%|████████  | 4/5 [00:03<00:00,  1.03it/s]
Epoch 6/50: |>train_loss: 13.421284103393555, val_loss: 14.375693321228027, val_accuracy: 0.21471038460731506
Original Image
100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).



100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
  0%|          | 0/5 [00:00<?, ?it/s]
Training started


100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.05it/s]
Epoch 0/50: |>train_loss: 13.325154685974121, val_loss: 10.803055763244629, val_accuracy: 0.28111666440963745
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.11it/s]
Epoch 1/50: |>train_loss: 13.542272186279297, val_loss: 10.805830001831055, val_accuracy: 0.28084033727645874
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.05it/s]

100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 2/50: |>train_loss: 13.509902954101562, val_loss: 10.778497695922852, val_accuracy: 0.2793063223361969
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 3/50: |>train_loss: 13.33478240966797, val_loss: 10.72920036315918, val_accuracy: 0.27710747718811035
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.04it/s]
Epoch 4/50: |>train_loss: 13.521278381347656, val_loss: 10.780816078186035, val_accuracy: 0.27406045794487
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Epoch 5/50: |>train_loss: 13.268630981445312, val_loss: 10.875605583190918, val_accuracy: 0.2660202383995056
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.04it/s]

100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 6/50: |>train_loss: 13.252777481079102, val_loss: 10.874167442321777, val_accuracy: 0.27213460206985474
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 7/50: |>train_loss: 13.492031478881836, val_loss: 10.851995468139648, val_accuracy: 0.27134180068969727
Original Image

 20%|██        | 1/5 [00:01<00:07,  1.95s/it]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
  0%|          | 0/5 [00:00<?, ?it/s]
Training started



100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
Training started


100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Epoch 0/50: |>train_loss: 13.525029182434082, val_loss: 11.275431632995605, val_accuracy: 0.25198936462402344
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.05it/s]

100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 1/50: |>train_loss: 13.481801795959473, val_loss: 11.331741333007812, val_accuracy: 0.2543591558933258
Original Image



 80%|████████  | 4/5 [00:04<00:01,  1.19s/it]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
  0%|          | 0/5 [00:00<?, ?it/s]
model size: 969.097MB
Training started


100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.04it/s]
Epoch 0/50: |>train_loss: 13.618274688720703, val_loss: 15.620160102844238, val_accuracy: 0.2094860076904297
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Epoch 1/50: |>train_loss: 13.675003814697266, val_loss: 15.589015007019043, val_accuracy: 0.21463243663311005
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:00<00:03,  1.04it/s]

100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 2/50: |>train_loss: 13.388618850708008, val_loss: 15.53692626953125, val_accuracy: 0.21894100308418274
Original Image
Epoch 4/50

100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 3/50: |>train_loss: 13.33902816772461, val_loss: 15.543293952941895, val_accuracy: 0.217640683054924
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.08it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 4/50: |>train_loss: 13.509998512268066, val_loss: 15.57411003112793, val_accuracy: 0.2170090228319168
Original Image


100%|██████████| 5/5 [00:04<00:00,  1.07it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 5/50: |>train_loss: 13.344384384155273, val_loss: 15.639938354492188, val_accuracy: 0.20977342128753662
Original Image
Epoch 7/50
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192]) torch.Size([45, 3, 256, 192]) torch.Size([5, 3, 256, 192])
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
  0%|          | 0/5 [00:00<?, ?it/s]
Training started


100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s]
Epoch 0/50: |>train_loss: 13.07110652923584, val_loss: 17.185300827026367, val_accuracy: 0.20480754971504211
Original Image
 60%|██████    | 3/5 [00:02<00:01,  1.04it/s]
100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Epoch 1/50: |>train_loss: 12.98589210510254, val_loss: 17.100637435913086, val_accuracy: 0.21425779163837433
Original Image
 20%|██        | 1/5 [00:00<00:03,  1.05it/s]
 60%|██████    | 3/5 [00:02<00:01,  1.03it/s]
100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
100%|██████████| 5/5 [00:04<00:00,  1.09it/s]
Epoch 2/50: |>train_loss: 12.91251049041748, val_loss: 17.100433349609375, val_accuracy: 0.21630115807056427
Original Image
 20%|██        | 1/5 [00:00<00:03,  1.04it/s]
 80%|████████  | 4/5 [00:03<00:00,  1.04it/s]
  0%|          | 0/5 [00:00<?, ?it/s].09it/s]
  0%|          | 0/5 [00:00<?, ?it/s].09it/s]
Epoch 3/50: |>train_loss: 13.189919471740723, val_loss: 17.09347915649414, val_accuracy: 0.21364803612232208
Original Image
 40%|████      | 2/5 [00:01<00:02,  1.04it/s]
 80%|████████  | 4/5 [00:04<00:01,  1.02s/it]
  0%|          | 0/5 [00:00<?, ?it/s].04it/s]
  0%|          | 0/5 [00:00<?, ?it/s].04it/s]
Epoch 4/50: |>train_loss: 12.87263526916504, val_loss: 17.09823989868164, val_accuracy: 0.21176204085350037
Original Image
 40%|████      | 2/5 [00:02<00:03,  1.13s/it]
 40%|████      | 2/5 [00:02<00:03,  1.13s/it]
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
 40%|████      | 2/5 [00:02<00:03,  1.13s/it]
model size: 969.097MB
Training started
Epoch 1/50
 40%|████      | 2/5 [00:02<00:03,  1.13s/it]
 40%|████      | 2/5 [00:02<00:03,  1.13s/it]
Epoch 0/50: |>train_loss: 13.364529418945313, val_loss: 17.128379821777344, val_accuracy: 0.1863950937986374
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 2/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 1/50: |>train_loss: 13.202044105529785, val_loss: 17.115652084350586, val_accuracy: 0.18388110399246216
Original Image
Epoch 3/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 2/50: |>train_loss: 13.153187370300293, val_loss: 17.156455993652344, val_accuracy: 0.17937977612018585
Original Image
Epoch 4/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 3/50: |>train_loss: 13.27226734161377, val_loss: 17.233234405517578, val_accuracy: 0.17473630607128143
Original Image
Epoch 5/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 4/50: |>train_loss: 13.295792579650879, val_loss: 17.295578002929688, val_accuracy: 0.1711941361427307
Original Image
Epoch 6/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 5/50: |>train_loss: 13.183577156066894, val_loss: 17.462646484375, val_accuracy: 0.16119824349880219
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 7/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
{"AutoImageProcessor": "type", "CoordAtt": "type", "DEVICE": "str", "DecoderBlock": "type", "Image": "module", "MobileNetV1Model": "type", "PIL": "module", "SEED": "int", "SSIM": "ABCMeta", "UnetWithAT": "type", "UnetWithoutAT": "type", "X_test": "Tensor", "X_train": "Tensor", "Y_test": "Tensor", "Y_train": "Tensor", "accuracy": "function", "buffer": "Tensor", "buffer_size": "int", "config": "Config", "folder_path_removed": "str", "folder_path_shadow": "str", "get_decoder_layers": "function", "get_encoder_layers": "function", "get_ipython": "function", "get_loss": "function", "h_sigmoid": "type", "h_swish": "type", "img_removed": "Tensor", "img_shadow": "Tensor", "load_images_from_folder": "function", "loss1": "L1Loss", "loss_layer": "function", "loss_weights": "tuple", "lpips": "module", "lpips_layer": "LPIPS", "model": "DataParallel", "n_images": "int", "nn": "module", "norm_mean": "list", "norm_std": "list", "np": "module", "optimizer": "Adam", "os": "module", "param": "Parameter", "param_size": "int", "plt": "module", "size_all_mb": "float", "torch": "module", "tqdm": "type", "train_dataset": "TensorDataset", "train_loader": "DataLoader", "train_test_split": "function", "training": "function", "transform": "Compose", "transforms": "module", "unnormalize_to_255": "function", "val_dataset": "TensorDataset", "val_loader": "DataLoader", "wandb": "module"}
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home2/sreenivas88/miniconda3/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
model size: 969.097MB
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Training started
Epoch 1/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 0/50: |>train_loss: 13.229573059082032, val_loss: 17.054283142089844, val_accuracy: 0.20986764132976532
Original Image
Epoch 2/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 1/50: |>train_loss: 13.046442604064941, val_loss: 17.001474380493164, val_accuracy: 0.20877952873706818
Original Image
Epoch 3/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 2/50: |>train_loss: 13.596335411071777, val_loss: 17.01069450378418, val_accuracy: 0.20124630630016327
Original Image
Epoch 4/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 3/50: |>train_loss: 13.14510440826416, val_loss: 17.00576400756836, val_accuracy: 0.19668854773044586
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 5/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 4/50: |>train_loss: 13.343742561340331, val_loss: 17.092761993408203, val_accuracy: 0.1890883445739746
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 6/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 5/50: |>train_loss: 13.120932960510254, val_loss: 17.147567749023438, val_accuracy: 0.18126198649406433
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 6/50: |>train_loss: 13.078557968139648, val_loss: 17.22386360168457, val_accuracy: 0.17396175861358643
Original Image
Epoch 8/50
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 7/50: |>train_loss: 13.283236312866212, val_loss: 17.324172973632812, val_accuracy: 0.16875000298023224
Original Image
Epoch 9/50
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.03s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 8/50: |>train_loss: 13.100770950317383, val_loss: 17.338611602783203, val_accuracy: 0.16613860428333282
Original Image
 20%|██        | 1/5 [00:01<00:05,  1.27s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:05,  1.27s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:05,  1.27s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:05,  1.27s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 9/50: |>train_loss: 13.234412002563477, val_loss: 17.406633377075195, val_accuracy: 0.16318118572235107
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.04s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.04s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.04s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.04s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 10/50: |>train_loss: 13.157132339477538, val_loss: 17.421619415283203, val_accuracy: 0.15857084095478058
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 11/50: |>train_loss: 13.159691619873048, val_loss: 17.426937103271484, val_accuracy: 0.15958461165428162
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 12/50: |>train_loss: 13.136037063598632, val_loss: 17.428377151489258, val_accuracy: 0.15827685594558716
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 13/50: |>train_loss: 13.042774772644043, val_loss: 17.447696685791016, val_accuracy: 0.16062815487384796
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 14/50: |>train_loss: 13.331540298461913, val_loss: 17.40411376953125, val_accuracy: 0.15740182995796204
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 15/50: |>train_loss: 13.26896686553955, val_loss: 17.470190048217773, val_accuracy: 0.149031400680542
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 16/50: |>train_loss: 13.075552940368652, val_loss: 17.50505256652832, val_accuracy: 0.14666661620140076
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 17/50: |>train_loss: 13.274389457702636, val_loss: 17.481374740600586, val_accuracy: 0.1514923870563507
Original Image
  0%|          | 0/5 [00:00<?, ?it/s].06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s].06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s].06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s].06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 18/50: |>train_loss: 13.703914070129395, val_loss: 17.46613311767578, val_accuracy: 0.1567012369632721
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 19/50: |>train_loss: 13.458218574523926, val_loss: 17.44415855407715, val_accuracy: 0.16117513179779053
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 20/50: |>train_loss: 13.083422660827637, val_loss: 17.46218490600586, val_accuracy: 0.1546401083469391
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 22/50
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 21/50: |>train_loss: 13.375691604614257, val_loss: 17.420021057128906, val_accuracy: 0.13956405222415924
  0%|          | 0/5 [00:00<?, ?it/s].08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
  0%|          | 0/5 [00:00<?, ?it/s].08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 23/50
  0%|          | 0/5 [00:00<?, ?it/s].08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 22/50: |>train_loss: 13.173147392272949, val_loss: 17.448402404785156, val_accuracy: 0.15647538006305695
Original Image
Epoch 24/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 23/50: |>train_loss: 13.057981491088867, val_loss: 17.432144165039062, val_accuracy: 0.15647776424884796
Original Image
Epoch 25/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 24/50: |>train_loss: 13.267016792297364, val_loss: 17.433916091918945, val_accuracy: 0.15576039254665375
Original Image
Epoch 26/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 25/50: |>train_loss: 13.367898178100585, val_loss: 17.491174697875977, val_accuracy: 0.13851909339427948
Original Image
Epoch 27/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 26/50: |>train_loss: 13.144098663330078, val_loss: 17.412132263183594, val_accuracy: 0.15754258632659912
Original Image
Epoch 28/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 27/50: |>train_loss: 13.053316116333008, val_loss: 17.462549209594727, val_accuracy: 0.15219691395759583
Original Image
Epoch 29/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 28/50: |>train_loss: 13.212662315368652, val_loss: 17.48006248474121, val_accuracy: 0.15065744519233704
Original Image
Epoch 30/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 29/50: |>train_loss: 13.254153060913087, val_loss: 17.440711975097656, val_accuracy: 0.15644457936286926
Original Image
Epoch 31/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 30/50: |>train_loss: 13.439809417724609, val_loss: 17.492311477661133, val_accuracy: 0.15305157005786896
Original Image
Epoch 32/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 31/50: |>train_loss: 13.284019470214844, val_loss: 17.44394874572754, val_accuracy: 0.15216632187366486
Original Image
Epoch 33/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 32/50: |>train_loss: 13.243848419189453, val_loss: 17.429248809814453, val_accuracy: 0.1524055004119873
Original Image
Epoch 34/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 33/50: |>train_loss: 13.236397552490235, val_loss: 17.41542625427246, val_accuracy: 0.15648309886455536
Original Image
Epoch 35/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 34/50: |>train_loss: 13.071934700012207, val_loss: 17.456283569335938, val_accuracy: 0.15056245028972626
Original Image
Epoch 36/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 35/50: |>train_loss: 13.091590309143067, val_loss: 17.413043975830078, val_accuracy: 0.15625841915607452
Original Image
Epoch 37/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 36/50: |>train_loss: 13.075559425354005, val_loss: 17.427654266357422, val_accuracy: 0.14947496354579926
Original Image
Epoch 38/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 37/50: |>train_loss: 13.034992599487305, val_loss: 17.442501068115234, val_accuracy: 0.1551017463207245
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 38/50: |>train_loss: 13.16974048614502, val_loss: 17.422714233398438, val_accuracy: 0.1591252237558365
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 39/50: |>train_loss: 13.326042366027831, val_loss: 17.411354064941406, val_accuracy: 0.15736055374145508
Original Image
Epoch 41/50
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.06s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 40/50: |>train_loss: 13.125131225585937, val_loss: 17.411113739013672, val_accuracy: 0.15540564060211182
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 41/50: |>train_loss: 13.009313201904297, val_loss: 17.45384407043457, val_accuracy: 0.15429607033729553
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 42/50: |>train_loss: 13.200436019897461, val_loss: 17.42446517944336, val_accuracy: 0.15630356967449188
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 43/50: |>train_loss: 13.08183536529541, val_loss: 17.45073890686035, val_accuracy: 0.1536877602338791
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 44/50: |>train_loss: 13.527458000183106, val_loss: 17.45914649963379, val_accuracy: 0.1523919403553009
Original Image
Epoch 46/50
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.08s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 45/50: |>train_loss: 13.18992805480957, val_loss: 17.397438049316406, val_accuracy: 0.15299591422080994
Original Image
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 46/50: |>train_loss: 13.054918479919433, val_loss: 17.433456420898438, val_accuracy: 0.15413659811019897
Original Image
Epoch 48/50
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
 20%|██        | 1/5 [00:01<00:04,  1.07s/it]show with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 47/50: |>train_loss: 13.247525405883788, val_loss: 17.39563751220703, val_accuracy: 0.15662693977355957
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 48/50: |>train_loss: 13.059644508361817, val_loss: 17.46904945373535, val_accuracy: 0.15096162259578705
Original Image
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 50/50
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch 49/50: |>train_loss: 13.192873573303222, val_loss: 17.470897674560547, val_accuracy: 0.14728422462940216
Original Image
Training finished