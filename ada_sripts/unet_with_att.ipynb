{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet with and without Attention: Residual Block + Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoImageProcessor, MobileNetV1Model\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Set parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "SEED = 0\n",
    "\n",
    "# set seed for all possible random functions to ensure reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msreenivasbp03\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu DEVICE\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} DEVICE\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_layers():\n",
    "\t\"\"\"\n",
    "\tRetrieves the layers of the MobileNetV1 model for encoding images.\n",
    "\n",
    "\tReturns:\n",
    "\t\tmobilenet_seq_blocks (list): List containing the layers of the MobileNetV1 model\n",
    "\t\t\tdivided into blocks.\n",
    "\t\tconv_stem (torch.nn.Module): The stem convolutional layer of the MobileNetV1 model.\n",
    "\t\timage_processor (transformers.AutoImageProcessor): Pretrained image processor for\n",
    "\t\t\tMobileNetV1.\n",
    "\t\"\"\"\n",
    "\t# download the model\n",
    "\timage_processor = AutoImageProcessor.from_pretrained(\n",
    "\t\t\"google/mobilenet_v1_1.0_224\")\n",
    "\tmodel = MobileNetV1Model.from_pretrained(\"google/mobilenet_v1_1.0_224\")\n",
    "\n",
    "\tmobilenet_seq_blocks = []\n",
    "\t# block 1 will contain 4 layer of model.layer\n",
    "\tblock = nn.Sequential(*list(model.layer)[:2])\n",
    "\tmobilenet_seq_blocks.append(block)\n",
    "\t# print(\"-\"*30,\"\\n\\nblock 1:\", block)\n",
    "\n",
    "\tblock = nn.Sequential(*list(model.layer)[2:4])\n",
    "\tmobilenet_seq_blocks.append(block)\n",
    "\t# print(\"-\"*30,\"\\n\\nblock 2:\", block)\n",
    "\n",
    "\tblock = nn.Sequential(*list(model.layer)[4:8])\n",
    "\tmobilenet_seq_blocks.append(block)\n",
    "\t# print(\"-\"*30,\"\\n\\nblock 3:\", block)\n",
    "\n",
    "\tblock = nn.Sequential(*list(model.layer)[8:12])\n",
    "\tmobilenet_seq_blocks.append(block)\n",
    "\t# print(\"-\"*30,\"\\n\\nblock 4:\", block)\n",
    "\n",
    "\tblock = nn.Sequential(*list(model.layer)[12:])\n",
    "\tmobilenet_seq_blocks.append(block)\n",
    "\t# print(\"-\"*30,\"\\n\\nblock 5:\", block)\n",
    "\n",
    "\t# printing the input and output channels of the first and last layers of each block\n",
    "\t# for i, block in enumerate(mobilenet_seq_blocks):\n",
    "\t# \t# Extracting the first and last layers of the block\n",
    "\t# \tfirst_layer = block[0]\n",
    "\t# \tlast_layer = block[-1]\n",
    "\n",
    "\t# \t# Get the input and output channels of the first and last layers\n",
    "\t# \tinput_channel_first_layer = first_layer.convolution.in_channels\n",
    "\t# \toutput_channel_last_layer = last_layer.convolution.out_channels\n",
    "\n",
    "\t# \tprint(f\"Block {i + 1}:\")\n",
    "\t# \tprint(\"Input channel of the first layer:\", input_channel_first_layer)\n",
    "\t# \tprint(\"Output channel of the last layer:\", output_channel_last_layer)\n",
    "\n",
    "\treturn mobilenet_seq_blocks, model.conv_stem, image_processor\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, expansion=3, do_up_sampling=True):\n",
    "\t\t# print(\"Alert: skip connection is not implemented in the decoder block\")\n",
    "\t\t\"\"\"\n",
    "\t\tDecoder block module.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tin_channels (int): Number of input channels.\n",
    "\t\t\tout_channels (int): Number of output channels.\n",
    "\t\t\texpansion (int, optional): Expansion factor. Default is 3.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(DecoderBlock, self).__init__()\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\t\tself.cnn1 = nn.Conv2d(in_channels, in_channels *\n",
    "\t\t\t\t\t\t\t  expansion, kernel_size=1, stride=1)\n",
    "\t\tself.bnn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "\t\t# nearest neighbor x2\n",
    "\t\tself.do_up_sampling = do_up_sampling\n",
    "\t\tself.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "\t\t# DW conv/ c_in*exp x 5 x 5 x c_in*exp\n",
    "\t\tself.cnn2 = nn.Conv2d(in_channels*expansion, in_channels *\n",
    "\t\t\t\t\t\t\t  expansion, kernel_size=5, padding=2, stride=1)\n",
    "\t\tself.bnn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "\n",
    "\t\tself.cnn3 = nn.Conv2d(in_channels*expansion,\n",
    "\t\t\t\t\t\t\t  out_channels, kernel_size=1, stride=1)\n",
    "\t\tself.bnn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass through the decoder block.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx (torch.Tensor): Input tensor.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: Output tensor.\n",
    "\t\t\"\"\"\n",
    "\t\tx = self.cnn1(x)\n",
    "\t\tx = self.bnn1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\n",
    "\t\tif self.do_up_sampling:\n",
    "\t\t\tx = self.upsample(x)\n",
    "\n",
    "\t\tx = self.cnn2(x)\n",
    "\t\tx = self.bnn2(x)\n",
    "\t\tx = self.relu(x)\n",
    "\n",
    "\t\tx = self.cnn3(x)\n",
    "\t\tx = self.bnn3(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def get_decoder_layers(out_sizes=[512, 256, 128, 64, 32]):\n",
    "\tdecoder_blocks = []\n",
    "\tfor i, out_size in enumerate(out_sizes):\n",
    "\t\tif i == 0:\n",
    "\t\t\tdecoder_blocks.append(DecoderBlock(out_size*2, out_size))\n",
    "\t\telif i == len(out_sizes)-1:\n",
    "\t\t\tdecoder_blocks.append(DecoderBlock(\n",
    "\t\t\t\tout_size*4, out_size, do_up_sampling=False))\n",
    "\t\telse:\n",
    "\t\t\tdecoder_blocks.append(DecoderBlock(out_size*4, out_size))\n",
    "\treturn decoder_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_sigmoid(nn.Module):\n",
    "\tdef __init__(self, inplace=True):\n",
    "\t\tsuper(h_sigmoid, self).__init__()\n",
    "\t\tself.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "\tdef __init__(self, inplace=True):\n",
    "\t\tsuper(h_swish, self).__init__()\n",
    "\t\tself.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class CoordAtt(nn.Module):\n",
    "\tdef __init__(self, inp, oup, reduction=32):\n",
    "\t\tsuper(CoordAtt, self).__init__()\n",
    "\t\tself.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "\t\tself.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "\t\tmip = max(8, inp // reduction)\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(mip)\n",
    "\t\tself.act = h_swish()\n",
    "\n",
    "\t\tself.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\n",
    "\t\tn, c, h, w = x.size()\n",
    "\t\tx_h = self.pool_h(x)\n",
    "\t\tx_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "\t\ty = torch.cat([x_h, x_w], dim=2)\n",
    "\t\ty = self.conv1(y)\n",
    "\t\ty = self.bn1(y)\n",
    "\t\ty = self.act(y)\n",
    "\n",
    "\t\tx_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "\t\tx_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "\t\ta_h = self.conv_h(x_h).sigmoid()\n",
    "\t\ta_w = self.conv_w(x_w).sigmoid()\n",
    "\n",
    "\t\tout = identity * a_w * a_h\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unet without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetWithoutAT(nn.Module):\n",
    "\t\"\"\"\n",
    "\tU-Net model architecture.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, loss_weights=(10,5), lr=0.5):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the U-Net model.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(UnetWithoutAT, self).__init__()\n",
    "\t\tencoder_blocks, image_stem_layer, image_processor = get_encoder_layers()\n",
    "\t\tdecoder_blocks = get_decoder_layers()\n",
    "\n",
    "\t\tself.en1, self.en2, self.en3, self.en4, self.en5 = encoder_blocks\n",
    "\t\tself.de1, self.de2, self.de3, self.de4, self.de5 = decoder_blocks\n",
    "\n",
    "\t\tself.encoder_blocks = [self.en1, self.en2,\n",
    "\t\t\t\t\t\t\t   self.en3, self.en4, self.en5]\n",
    "\t\tself.decoder_blocks = [self.de1, self.de2,\n",
    "\t\t\t\t\t\t\t   self.de3, self.de4, self.de5]\n",
    "\n",
    "\t\tself.image_processor = image_processor\n",
    "\t\tself.image_stem_layer = image_stem_layer\n",
    "\t\t# print(\"Image stem layer:\", self.image_stem_layer)\n",
    "\t\tself.out_image_stem_layer = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(32, 3, kernel_size=3, stride=2,\n",
    "\t\t\t\t\t\t\t   bias=False, padding=1, output_padding=1),\n",
    "\t\t\tnn.BatchNorm2d(3, eps=0.001, momentum=0.9997,\n",
    "\t\t\t\t\t\t   affine=True, track_running_stats=True),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.loss_weights = loss_weights\n",
    "\t\tself.loss1 = nn.L1Loss()\n",
    "\t\t# self.loss2 = nn.\n",
    "\n",
    "\t\tself.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\tdef forward(self, x, process_image=False):\n",
    "\t\t\"\"\"\n",
    "\t\tPerforms forward pass through the U-Net model.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx (torch.Tensor): Input image tensor.\n",
    "\t\t\tprocess_image (bool): Whether to preprocess input image.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: Output image tensor.\n",
    "\t\t\"\"\"\n",
    "\t\tassert isinstance(x, torch.Tensor), \"Input should be a tensor\"\n",
    "\t\tif process_image:\n",
    "\t\t\tnew_x = [self.image_processor(img)['pixel_values'][0] for img in x]\n",
    "\t\t\tx = torch.stack(new_x).permute(0, 3, 1, 2)\n",
    "\t\tassert x.shape[1] == 3, \"Input image should have 3 channels(nx3x224x224)\"\n",
    "\n",
    "\t\ttemp_in_x = x\n",
    "\t\tx = self.image_stem_layer(x)\n",
    "\t\tenc_outputs = []\n",
    "\t\tfor indx, enc_block in enumerate(self.encoder_blocks):\n",
    "\t\t\tx = enc_block(x)\n",
    "\t\t\t# print(f\"Encoder block {indx} output shape: {x.shape}\")\n",
    "\t\t\tenc_outputs.append(x)\n",
    "\t\tfor indx, dec_block in enumerate(self.decoder_blocks):\n",
    "\t\t\tif indx == 0:\n",
    "\t\t\t\tx = dec_block(x)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = dec_block(\n",
    "\t\t\t\t\ttorch.cat([x, enc_outputs[len(self.decoder_blocks) - indx - 1]], dim=1))\n",
    "\t\t\t# print(f\"Decoder block {indx} output shape: {x.shape}\")\n",
    "\t\treturn 0.5 * self.out_image_stem_layer(x) + temp_in_x\n",
    "\t\n",
    "\tdef loss_layer(self,y_pred,y):\n",
    "\t\t'''\n",
    "\t\tConsidering wieghted loss\n",
    "\t\t'''\n",
    "\t\treturn self.loss_weights[0]*self.loss1(y_pred, y)\n",
    "\t\n",
    "\tdef loss(self,x,y):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred = self(x)\n",
    "\t\t\treturn self.loss_layer(pred,y).item()\n",
    "\t\n",
    "\tdef accuracy(self,x,y):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred = self(x)\n",
    "\t\t\tmetric = SSIM()\n",
    "\t\t\tif DEVICE=='cuda':\n",
    "\t\t\t\tmetric = metric.cuda()\n",
    "\t\t\treturn metric(pred,y)\n",
    "\t\n",
    "\tdef fit(self, x_train, y_train, batch_size=32, n_epochs=3, validation=False, x_val=None, y_val=None):\n",
    "\t\t'''\n",
    "\t\t\n",
    "\t\t'''\n",
    "\t\t'''\n",
    "\t\twandb.log(metrics)\n",
    "\n",
    "\t\t# store best model\n",
    "\t\t\n",
    "\twandb.finish()\n",
    "\t\t'''\n",
    "\t\tfor epoch in tqdm(range(n_epochs)):\n",
    "\t\t\t#mini-batch gradient descent\n",
    "\t\t\tfor i in range((x_train.shape[0] - 1) // batch_size + 1):\n",
    "\t\t\t\tstart_i = i * batch_size\n",
    "\t\t\t\tend_i = start_i + batch_size\n",
    "\t\t\t\txb = x_train[start_i:end_i]\n",
    "\t\t\t\tyb = y_train[start_i:end_i]\n",
    "\t\t\t\tpred = self(xb)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss = self.loss_layer(pred,yb)\n",
    "\t\t\t   \n",
    "\t\t\t\tmetrices = {\"train/loss\": loss.item(),\n",
    "\t\t\t\t\t\t   \"epoch\": epoch+1, \n",
    "\t\t\t\t}\n",
    "\t\t\t\tif validation:\n",
    "\t\t\t\t\tmetrices['val/loss'] = self.loss(x_val,y_val)\n",
    "\t\t\t\t\tmetrices['val/accuracy'] = self.accuracy(x_val,y_val)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\twandb.log(metrices)\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\t\n",
    "\t\twandb.finish()\n",
    "\t\treturn self.loss(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetWithAT(nn.Module):\n",
    "\t\"\"\"\n",
    "\tU-Net model architecture.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, loss_weights=(10,5), lr=0.5):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the U-Net model.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(UnetWithoutAT, self).__init__()\n",
    "\t\tencoder_blocks, image_stem_layer, image_processor = get_encoder_layers()\n",
    "\t\tdecoder_blocks = get_decoder_layers()\n",
    "\n",
    "\t\tself.en1, self.en2, self.en3, self.en4, self.en5 = encoder_blocks\n",
    "\t\tself.de1, self.de2, self.de3, self.de4, self.de5 = decoder_blocks\n",
    "\n",
    "\t\tself.encoder_blocks = [self.en1, self.en2,\n",
    "\t\t\t\t\t\t\t   self.en3, self.en4, self.en5]\n",
    "\t\tself.decoder_blocks = [self.de1, self.de2,\n",
    "\t\t\t\t\t\t\t   self.de3, self.de4, self.de5]\n",
    "\n",
    "\t\tself.lra_att = CoordAtt(3, 3)\n",
    "\t\tself.ldra_att = CoordAtt(3, 3)\n",
    "\t\tself.image_processor = image_processor\n",
    "\t\tself.image_stem_layer = image_stem_layer\n",
    "\t\t# print(\"Image stem layer:\", self.image_stem_layer)\n",
    "\t\tself.out_image_stem_layer = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(32, 3, kernel_size=3, stride=2,\n",
    "\t\t\t\t\t\t\t   bias=False, padding=1, output_padding=1),\n",
    "\t\t\tnn.BatchNorm2d(3, eps=0.001, momentum=0.9997,\n",
    "\t\t\t\t\t\t   affine=True, track_running_stats=True),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.loss_weights = loss_weights\n",
    "\t\tself.loss1 = nn.L1Loss()\n",
    "\t\t# self.loss2 = nn.\n",
    "\n",
    "\t\tself.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\tdef forward(self, x, process_image=False):\n",
    "\t\t\"\"\"\n",
    "\t\tPerforms forward pass through the U-Net model.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx (torch.Tensor): Input image tensor.\n",
    "\t\t\tprocess_image (bool): Whether to preprocess input image.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: Output image tensor.\n",
    "\t\t\"\"\"\n",
    "\t\tassert isinstance(x, torch.Tensor), \"Input should be a tensor\"\n",
    "\t\tif process_image:\n",
    "\t\t\tnew_x = [self.image_processor(img)['pixel_values'][0] for img in x]\n",
    "\t\t\tx = torch.stack(new_x).permute(0, 3, 1, 2)\n",
    "\t\tassert x.shape[1] == 3, \"Input image should have 3 channels(nx3x224x224)\"\n",
    "\n",
    "\t\ttemp_in_x = x\n",
    "\t\tx = self.image_stem_layer(x)\n",
    "\t\tenc_outputs = []\n",
    "\t\tfor indx, enc_block in enumerate(self.encoder_blocks):\n",
    "\t\t\tx = enc_block(x)\n",
    "\t\t\t# print(f\"Encoder block {indx} output shape: {x.shape}\")\n",
    "\t\t\tenc_outputs.append(x)\n",
    "\t\tfor indx, dec_block in enumerate(self.decoder_blocks):\n",
    "\t\t\tif indx == 0:\n",
    "\t\t\t\tx = dec_block(x)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = dec_block(\n",
    "\t\t\t\t\ttorch.cat([x, enc_outputs[len(self.decoder_blocks) - indx - 1]], dim=1))\n",
    "\t\t\t# print(f\"Decoder block {indx} output shape: {x.shape}\")\n",
    "\n",
    "\t\tx = self.out_image_stem_layer(x)\n",
    "\n",
    "\t\t# att layer\n",
    "\t\ttemp_in_x = self.lra_att(temp_in_x)\n",
    "\t\tx = self.ldra_att(x)\n",
    "\n",
    "\t\treturn x + temp_in_x\n",
    "\t\n",
    "\tdef loss_layer(self,y_pred,y):\n",
    "\t\t'''\n",
    "\t\tConsidering wieghted loss\n",
    "\t\t'''\n",
    "\t\treturn self.loss_weights[0]*self.loss1(y_pred, y)\n",
    "\t\n",
    "\tdef loss(self,x,y):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred = self(x)\n",
    "\t\t\treturn self.loss_layer(pred,y).item()\n",
    "\t\n",
    "\tdef accuracy(self,x,y):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred = self(x)\n",
    "\t\t\tmetric = SSIM()\n",
    "\t\t\tif DEVICE=='cuda':\n",
    "\t\t\t\tmetric = metric.cuda()\n",
    "\t\t\treturn metric(pred,y)\n",
    "\t\n",
    "\tdef fit(self, x_train, y_train, batch_size=32, n_epochs=3, validation=False, x_val=None, y_val=None):\n",
    "\t\t'''\n",
    "\t\t\n",
    "\t\t'''\n",
    "\t\t'''\n",
    "\t\twandb.log(metrics)\n",
    "\n",
    "\t\t# store best model\n",
    "\t\t\n",
    "\twandb.finish()\n",
    "\t\t'''\n",
    "\t\tfor epoch in tqdm(range(n_epochs)):\n",
    "\t\t\t#mini-batch gradient descent\n",
    "\t\t\tfor i in range((x_train.shape[0] - 1) // batch_size + 1):\n",
    "\t\t\t\tstart_i = i * batch_size\n",
    "\t\t\t\tend_i = start_i + batch_size\n",
    "\t\t\t\txb = x_train[start_i:end_i]\n",
    "\t\t\t\tyb = y_train[start_i:end_i]\n",
    "\t\t\t\tpred = self(xb)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss = self.loss_layer(pred,yb)\n",
    "\t\t\t   \n",
    "\t\t\t\tmetrices = {\"train/loss\": loss.item(),\n",
    "\t\t\t\t\t\t   \"epoch\": epoch+1, \n",
    "\t\t\t\t}\n",
    "\t\t\t\tif validation:\n",
    "\t\t\t\t\tmetrices['val/loss'] = self.loss(x_val,y_val)\n",
    "\t\t\t\t\tmetrices['val/accuracy'] = self.accuracy(x_val,y_val)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\twandb.log(metrices)\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\t\n",
    "\t\twandb.finish()\n",
    "\t\treturn self.loss(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(384, 512)\n",
      "10\n",
      "(384, 512)\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "\timages = []\n",
    "\tfor filename in os.listdir(folder_path):\n",
    "\t\tif filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "\t\t\timage_path = os.path.join(folder_path, filename)\n",
    "\t\t\timg = Image.open(image_path)\n",
    "\t\t\timages.append(img)\n",
    "\treturn images\n",
    "\n",
    "# folder_path_shadow = f\"./input\"\n",
    "# folder_path_removed = f\"./target\"\n",
    "\n",
    "folder_path_shadow = f\"../test_images/input\"\n",
    "folder_path_removed = f\"../test_images/target\"\n",
    "\n",
    "n_images = 10\n",
    "img_shadow = load_images_from_folder(folder_path_shadow)[:n_images]\n",
    "img_removed = load_images_from_folder(folder_path_removed)[:n_images]\n",
    "\n",
    "print(len(img_shadow),flush=True)\n",
    "print(img_shadow[0].size,flush=True)\n",
    "\n",
    "print(len(img_removed),flush=True)\n",
    "print(img_removed[0].size,flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((256,192)),  # Ensure the size\n",
    "\ttransforms.ToTensor(),          # Convert images to tensors\n",
    "])\n",
    "\n",
    "img_shadow = [transform(img) for img in img_shadow]\n",
    "img_shadow = torch.stack(img_shadow)\n",
    "\n",
    "img_removed = [transform(img) for img in img_removed]\n",
    "img_removed = torch.stack(img_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3, 256, 192]) torch.Size([1, 3, 256, 192])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(img_shadow, img_removed, test_size=0.1)\n",
    "print(X_train.shape,Y_test.shape,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27d7cvye) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-dragon-6</strong> at: <a href='https://wandb.ai/sreenivasbp03/smai-proj-unet/runs/27d7cvye' target=\"_blank\">https://wandb.ai/sreenivasbp03/smai-proj-unet/runs/27d7cvye</a><br/> View project at: <a href='https://wandb.ai/sreenivasbp03/smai-proj-unet' target=\"_blank\">https://wandb.ai/sreenivasbp03/smai-proj-unet</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240419_001146-27d7cvye\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27d7cvye). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Administrator\\OneDrive - International Institute of Information Technology\\Courses\\sem_6\\files\\SMAI\\project\\LP-IOANet\\ada_sripts\\wandb\\run-20240419_001223-r4pcbnqq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sreenivasbp03/smai-proj-unet/runs/r4pcbnqq' target=\"_blank\">glad-shape-7</a></strong> to <a href='https://wandb.ai/sreenivasbp03/smai-proj-unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sreenivasbp03/smai-proj-unet' target=\"_blank\">https://wandb.ai/sreenivasbp03/smai-proj-unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sreenivasbp03/smai-proj-unet/runs/r4pcbnqq' target=\"_blank\">https://wandb.ai/sreenivasbp03/smai-proj-unet/runs/r4pcbnqq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DEVICE=='cuda':\n",
    "\tX_train = X_train.cuda()\n",
    "\tY_train = Y_train.cuda()\n",
    "\t\n",
    "\tX_test = X_test.cuda()\n",
    "\tY_test = Y_test.cuda()\n",
    "\t\n",
    "wandb.init(project=\"smai-proj-unet\",config={\n",
    "\t\"dataset\": \"Shadoc-lowres\",\n",
    "\t\"architecture\": \"UNetWithoutAT\",\n",
    "\t\n",
    "\t\"epochs\":10,\n",
    "\t\"learning_rate\" : 5e-1,\n",
    "\t\"batch_size\": 16,\n",
    "\t  })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetWithoutAT(lr=config.learning_rate)\n",
    "if DEVICE=='cuda':\n",
    "\tmodel = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=config.batch_size, n_epochs=config.epochs, validation=True, x_val=X_test, y_val=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
